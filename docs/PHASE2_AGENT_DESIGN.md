# ğŸ¤– Phase 2 Agentç³»ç»Ÿè¯¦ç»†è®¾è®¡

**Multi-Agentåä½œæ¶æ„è®¾è®¡æ–‡æ¡£**

---

## ğŸ“‹ ç›®å½•

1. [æ¶æ„æ¦‚è§ˆ](#æ¶æ„æ¦‚è§ˆ)
2. [Agent 1: PreprocessAgent](#agent-1-preprocessagent)
3. [Agent 2: UnifiedGradingAgent](#agent-2-unifiedgradingagent)
4. [Agent 3: LocationAnnotationAgent](#agent-3-locationannotationagent)
5. [GradingOrchestrator](#gradingorchestrator)
6. [æ•°æ®æµ](#æ•°æ®æµ)
7. [é”™è¯¯å¤„ç†](#é”™è¯¯å¤„ç†)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## æ¶æ„æ¦‚è§ˆ

### Multi-Agentåä½œæµç¨‹

```
ç”¨æˆ·æäº¤ä½œä¸š
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      GradingOrchestrator (LangGraph)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”œâ”€â†’ [Agent 1] PreprocessAgent
    â”‚   è¾“å…¥: ä½œä¸šå›¾ç‰‡
    â”‚   è¾“å‡º: é¢˜ç›®åˆ—è¡¨ + è¾¹ç•Œæ¡†
    â”‚   æŠ€æœ¯: Tesseract OCR
    â”‚   æˆæœ¬: $0.001/é¡µ
    â”‚
    â”œâ”€â†’ [Agent 2] UnifiedGradingAgent
    â”‚   è¾“å…¥: é¢˜ç›®æˆªå›¾
    â”‚   è¾“å‡º: åˆ†æ•° + é”™è¯¯ + åé¦ˆ
    â”‚   æŠ€æœ¯: Gemini 2.5 Flash Lite
    â”‚   æˆæœ¬: $0.000043/é¢˜
    â”‚
    â””â”€â†’ [Agent 3] LocationAnnotationAgent â­ æ–°å¢
        è¾“å…¥: é¢˜ç›®å›¾ç‰‡ + é”™è¯¯æè¿°
        è¾“å‡º: ç²¾ç¡®åƒç´ åæ ‡ + ç½®ä¿¡åº¦
        æŠ€æœ¯: Gemini 2.5 Flash Lite
        æˆæœ¬: $0.000001/é¢˜
    â†“
ä¿å­˜åˆ°æ•°æ®åº“ + WebSocketæ¨é€
```

### LangGraphçŠ¶æ€æœº

```python
from langgraph.graph import StateGraph

# çŠ¶æ€å®šä¹‰
class GradingState(TypedDict):
    submission_id: str
    images: List[str]
    
    # Agent 1 è¾“å‡º
    question_segments: Optional[List[QuestionSegment]]
    
    # Agent 2 è¾“å‡º
    grading_results: Optional[List[QuestionGrading]]
    
    # Agent 3 è¾“å‡º
    annotated_results: Optional[List[AnnotatedGrading]]
    
    # æœ€ç»ˆç»“æœ
    final_result: Optional[GradingResult]
    
    # é”™è¯¯ä¿¡æ¯
    error: Optional[str]

# å·¥ä½œæµå®šä¹‰
workflow = StateGraph(GradingState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("preprocess", preprocess_node)
workflow.add_node("grade", grade_node)
workflow.add_node("annotate", annotate_node)
workflow.add_node("finalize", finalize_node)

# å®šä¹‰æµç¨‹
workflow.add_edge("__start__", "preprocess")
workflow.add_edge("preprocess", "grade")
workflow.add_edge("grade", "annotate")
workflow.add_edge("annotate", "finalize")
workflow.add_edge("finalize", "__end__")

# ç¼–è¯‘
graph = workflow.compile()
```

---

## Agent 1: PreprocessAgent

### èŒè´£

**é¢˜ç›®åˆ†æ®µè¯†åˆ«** - å°†ä½œä¸šå›¾ç‰‡åˆ†å‰²æˆç‹¬ç«‹çš„é¢˜ç›®

### è¾“å…¥

```python
class PreprocessInput(BaseModel):
    images: List[str]  # å›¾ç‰‡URLåˆ—è¡¨
    image_metadata: List[ImageMetadata]  # å›¾ç‰‡å…ƒæ•°æ®
```

### è¾“å‡º

```python
class QuestionSegment(BaseModel):
    question_number: str        # "ç¬¬1é¢˜" æˆ– "1." æˆ– "(1)"
    question_index: int         # 0-basedç´¢å¼•
    page_index: int             # æ‰€åœ¨é¡µé¢ç´¢å¼•
    bbox: BoundingBox           # é¢˜ç›®è¾¹ç•Œæ¡†
    cropped_image: str          # é¢˜ç›®æˆªå›¾URL
    ocr_text: str               # OCRè¯†åˆ«çš„æ–‡å­—
    confidence: float           # è¯†åˆ«ç½®ä¿¡åº¦ (0-1)
```

### å®ç°æ–¹æ¡ˆ

#### 1. OCRè¯†åˆ«

```python
class PreprocessAgent:
    def __init__(self):
        self.ocr_engine = TesseractOCR(lang='chi_sim+eng')
    
    async def recognize_text(self, image_url: str) -> OCRResult:
        """OCRæ–‡å­—è¯†åˆ«"""
        image = await self._load_image(image_url)
        
        # é¢„å¤„ç†å›¾ç‰‡
        processed_image = self._preprocess_image(image)
        
        # OCRè¯†åˆ«
        result = self.ocr_engine.image_to_data(
            processed_image,
            output_type=pytesseract.Output.DICT
        )
        
        return OCRResult(
            text=result['text'],
            boxes=result['boxes'],
            confidences=result['conf']
        )
    
    def _preprocess_image(self, image):
        """å›¾ç‰‡é¢„å¤„ç†"""
        # 1. ç°åº¦åŒ–
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # 2. äºŒå€¼åŒ–
        _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        # 3. å»å™ª
        denoised = cv2.fastNlMeansDenoising(binary)
        
        return denoised
```

#### 2. é¢˜å·è¯†åˆ«

```python
def _detect_question_markers(self, ocr_result: OCRResult) -> List[QuestionMarker]:
    """è¯†åˆ«é¢˜å·"""
    markers = []
    
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…é¢˜å·
    patterns = [
        r'^(\d+)[.ã€)]',           # 1. æˆ– 1ã€ æˆ– 1)
        r'^[(ï¼ˆ](\d+)[)ï¼‰]',       # (1) æˆ– ï¼ˆ1ï¼‰
        r'^ç¬¬(\d+)é¢˜',             # ç¬¬1é¢˜
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[.ã€)]',  # ä¸€ã€ æˆ– äºŒã€
    ]
    
    for i, text in enumerate(ocr_result.text):
        for pattern in patterns:
            match = re.match(pattern, text.strip())
            if match:
                markers.append(QuestionMarker(
                    text=text,
                    number=self._extract_number(match),
                    bbox=ocr_result.boxes[i],
                    confidence=ocr_result.confidences[i]
                ))
                break
    
    return markers
```

#### 3. è¾¹ç•Œæ¡†è®¡ç®—

```python
def _calculate_bbox(
    self,
    marker: QuestionMarker,
    next_marker: Optional[QuestionMarker],
    ocr_result: OCRResult,
    image_height: int
) -> BoundingBox:
    """è®¡ç®—é¢˜ç›®è¾¹ç•Œæ¡†"""
    
    # èµ·å§‹ä½ç½®: é¢˜å·çš„yåæ ‡
    start_y = marker.bbox.y
    
    # ç»“æŸä½ç½®: ä¸‹ä¸€ä¸ªé¢˜å·çš„yåæ ‡ æˆ– å›¾ç‰‡åº•éƒ¨
    if next_marker:
        end_y = next_marker.bbox.y
    else:
        end_y = image_height
    
    # å·¦å³è¾¹ç•Œ: æ•´ä¸ªå›¾ç‰‡å®½åº¦
    start_x = 0
    end_x = ocr_result.image_width
    
    # æ·»åŠ è¾¹è·
    padding = 10
    
    return BoundingBox(
        x=max(0, start_x - padding),
        y=max(0, start_y - padding),
        width=min(end_x + padding, ocr_result.image_width) - start_x,
        height=end_y - start_y + padding
    )
```

#### 4. é¢˜ç›®æˆªå›¾

```python
async def _crop_question(
    self,
    image_url: str,
    bbox: BoundingBox
) -> str:
    """æˆªå–é¢˜ç›®å›¾ç‰‡"""
    image = await self._load_image(image_url)
    
    # è£å‰ª
    cropped = image[
        bbox.y:bbox.y + bbox.height,
        bbox.x:bbox.x + bbox.width
    ]
    
    # ä¿å­˜åˆ°äº‘å­˜å‚¨
    cropped_url = await self.storage.upload(cropped, f"questions/{uuid4()}.jpg")
    
    return cropped_url
```

### å®Œæ•´æµç¨‹

```python
async def segment_questions(
    self,
    images: List[str]
) -> List[QuestionSegment]:
    """é¢˜ç›®åˆ†æ®µè¯†åˆ«"""
    segments = []
    question_index = 0
    
    for page_index, image_url in enumerate(images):
        # 1. OCRè¯†åˆ«
        ocr_result = await self.recognize_text(image_url)
        
        # 2. è¯†åˆ«é¢˜å·
        markers = self._detect_question_markers(ocr_result)
        
        # 3. è®¡ç®—è¾¹ç•Œæ¡†å’Œæˆªå›¾
        for i, marker in enumerate(markers):
            next_marker = markers[i + 1] if i + 1 < len(markers) else None
            
            bbox = self._calculate_bbox(
                marker,
                next_marker,
                ocr_result,
                ocr_result.image_height
            )
            
            cropped_image = await self._crop_question(image_url, bbox)
            
            ocr_text = self._extract_text_in_bbox(ocr_result, bbox)
            
            segments.append(QuestionSegment(
                question_number=marker.text,
                question_index=question_index,
                page_index=page_index,
                bbox=bbox,
                cropped_image=cropped_image,
                ocr_text=ocr_text,
                confidence=marker.confidence
            ))
            
            question_index += 1
    
    return segments
```

### æ€§èƒ½ä¼˜åŒ–

1. **å¹¶è¡Œå¤„ç†**: å¤šé¡µå›¾ç‰‡å¹¶è¡ŒOCR
2. **ç¼“å­˜OCRç»“æœ**: é¿å…é‡å¤è¯†åˆ«
3. **å›¾ç‰‡é¢„å¤„ç†**: æé«˜OCRå‡†ç¡®åº¦
4. **æ‰¹é‡ä¸Šä¼ **: æ‰¹é‡ä¸Šä¼ æˆªå›¾åˆ°äº‘å­˜å‚¨

### é”™è¯¯å¤„ç†

```python
async def segment_questions_with_fallback(
    self,
    images: List[str]
) -> List[QuestionSegment]:
    """å¸¦å…œåº•çš„é¢˜ç›®åˆ†æ®µ"""
    try:
        segments = await self.segment_questions(images)
        
        # éªŒè¯ç»“æœ
        if len(segments) == 0:
            raise ValueError("æœªè¯†åˆ«åˆ°ä»»ä½•é¢˜ç›®")
        
        # æ£€æŸ¥ç½®ä¿¡åº¦
        low_confidence = [s for s in segments if s.confidence < 0.5]
        if len(low_confidence) > len(segments) * 0.3:
            logger.warning(f"ä½ç½®ä¿¡åº¦é¢˜ç›®è¿‡å¤š: {len(low_confidence)}/{len(segments)}")
        
        return segments
        
    except Exception as e:
        logger.error(f"é¢˜ç›®åˆ†æ®µå¤±è´¥: {e}")
        
        # å…œåº•æ–¹æ¡ˆ: å°†æ•´é¡µä½œä¸ºä¸€ä¸ªé¢˜ç›®
        return self._fallback_segment(images)
```

---

## Agent 2: UnifiedGradingAgent

### èŒè´£

**æ‰¹æ”¹è¯†åˆ«** - è¯†åˆ«é”™è¯¯ã€è®¡ç®—åˆ†æ•°ã€ç”Ÿæˆåé¦ˆ

### è¾“å…¥

```python
class GradingInput(BaseModel):
    question_segment: QuestionSegment
    standard_answer: Optional[str]
    rubric: Optional[Dict]
```

### è¾“å‡º

```python
class QuestionGrading(BaseModel):
    question_index: int
    score: float
    max_score: float
    status: str  # "correct" | "warning" | "error"
    
    errors: List[ErrorItem]
    correct_parts: List[CorrectItem]
    warnings: List[WarningItem]
    
    feedback: str
```

### å®ç°

è¯¦è§Phase 1æ–‡æ¡£ï¼Œè¿™é‡Œä¸å†é‡å¤ã€‚

---

## Agent 3: LocationAnnotationAgent

### èŒè´£

**ç²¾ç¡®ä½ç½®æ ‡æ³¨** - åœ¨å›¾ç‰‡ä¸­ç²¾ç¡®å®šä½é”™è¯¯ä½ç½®

### è¾“å…¥

```python
class LocationInput(BaseModel):
    image_url: str
    image_width: int
    image_height: int
    question_bbox: BoundingBox
    error: ErrorItem
```

### è¾“å‡º

```python
class LocationOutput(BaseModel):
    bbox: BoundingBox
    type: str  # "point" | "line" | "area"
    confidence: float  # 0-1
    reasoning: str
```

### Promptè®¾è®¡

```python
LOCATION_PROMPT_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä½ç½®æ ‡æ³¨ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åœ¨å­¦ç”Ÿä½œä¸šå›¾ç‰‡ä¸­ç²¾ç¡®å®šä½é”™è¯¯ä½ç½®ã€‚

## å›¾ç‰‡ä¿¡æ¯
- å›¾ç‰‡å°ºå¯¸: {width}px Ã— {height}px
- é¢˜ç›®åŒºåŸŸ: x={bbox_x}, y={bbox_y}, width={bbox_width}, height={bbox_height}

## é”™è¯¯ä¿¡æ¯
- é”™è¯¯ç±»å‹: {error_type}
- é”™è¯¯æè¿°: {error_description}
- ç›¸å…³æ–‡å­—: {related_text}

## ä»»åŠ¡è¦æ±‚
è¯·ä»”ç»†è§‚å¯Ÿå›¾ç‰‡ï¼Œæ‰¾åˆ°é”™è¯¯æ‰€åœ¨çš„ä½ç½®ï¼Œå¹¶è¿”å›JSONæ ¼å¼:

{{
  "bbox": {{
    "x": 150,        // å·¦ä¸Šè§’xåæ ‡ (åƒç´ ï¼Œç›¸å¯¹äºæ•´å¼ å›¾ç‰‡)
    "y": 200,        // å·¦ä¸Šè§’yåæ ‡ (åƒç´ ï¼Œç›¸å¯¹äºæ•´å¼ å›¾ç‰‡)
    "width": 200,    // å®½åº¦ (åƒç´ )
    "height": 50     // é«˜åº¦ (åƒç´ )
  }},
  "type": "area",    // æ ‡æ³¨ç±»å‹: point(ç‚¹), line(çº¿), area(åŒºåŸŸ)
  "confidence": 0.95, // ç½®ä¿¡åº¦ (0-1)
  "reasoning": "é”™è¯¯ä½äºç¬¬äºŒè¡Œçš„è®¡ç®—ç»“æœéƒ¨åˆ†ï¼Œx+2y=8çš„æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œxçš„ç³»æ•°è®¡ç®—é”™è¯¯"
}}

## å®šä½è¦æ±‚
1. åæ ‡å¿…é¡»æ˜¯ç›¸å¯¹äºæ•´å¼ å›¾ç‰‡çš„ç»å¯¹åƒç´ åæ ‡
2. bboxåº”è¯¥ç´§å¯†åŒ…å›´é”™è¯¯å†…å®¹ï¼Œä¸è¦è¿‡å¤§æˆ–è¿‡å°
3. å¦‚æœæ— æ³•å‡†ç¡®å®šä½ï¼Œconfidenceè®¾ä¸º0.5ä»¥ä¸‹
4. reasoningå¿…é¡»è¯¦ç»†è¯´æ˜å®šä½ä¾æ®ï¼ŒåŒ…æ‹¬é”™è¯¯åœ¨ç¬¬å‡ è¡Œã€ç¬¬å‡ æ­¥ç­‰

## æ³¨æ„äº‹é¡¹
- åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—
- ç¡®ä¿åæ ‡åœ¨å›¾ç‰‡èŒƒå›´å†…
- ç¡®ä¿bboxåœ¨é¢˜ç›®åŒºåŸŸå†…æˆ–é™„è¿‘
"""
```

### å®ç°

```python
class LocationAnnotationAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="google/gemini-2.5-flash-lite",
            base_url="https://openrouter.ai/api/v1",
            api_key=os.getenv("OPENROUTER_API_KEY"),
            temperature=0.1,  # ä½æ¸©åº¦ï¼Œæé«˜å‡†ç¡®æ€§
        )
    
    async def annotate(
        self,
        input: LocationInput
    ) -> LocationOutput:
        """ç²¾ç¡®ä½ç½®æ ‡æ³¨"""
        
        # 1. æ„å»ºPrompt
        prompt = self._build_prompt(input)
        
        # 2. è°ƒç”¨LLM
        response = await self.llm.ainvoke([
            HumanMessage(content=[
                {
                    "type": "text",
                    "text": prompt
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": input.image_url
                    }
                }
            ])
        ])
        
        # 3. è§£æå“åº”
        result = self._parse_response(response.content)
        
        # 4. éªŒè¯ç»“æœ
        validated_result = self._validate_result(result, input)
        
        # 5. å…œåº•å¤„ç†
        if validated_result.confidence < 0.5:
            validated_result = self._get_fallback_location(input)
        
        return validated_result
    
    def _build_prompt(self, input: LocationInput) -> str:
        """æ„å»ºPrompt"""
        return LOCATION_PROMPT_TEMPLATE.format(
            width=input.image_width,
            height=input.image_height,
            bbox_x=input.question_bbox.x,
            bbox_y=input.question_bbox.y,
            bbox_width=input.question_bbox.width,
            bbox_height=input.question_bbox.height,
            error_type=input.error.type,
            error_description=input.error.description,
            related_text=input.error.related_text or "æ— "
        )
    
    def _parse_response(self, content: str) -> LocationOutput:
        """è§£æLLMå“åº”"""
        try:
            # æå–JSON
            json_match = re.search(r'\{.*\}', content, re.DOTALL)
            if not json_match:
                raise ValueError("æœªæ‰¾åˆ°JSON")
            
            data = json.loads(json_match.group())
            
            return LocationOutput(
                bbox=BoundingBox(**data['bbox']),
                type=data['type'],
                confidence=data['confidence'],
                reasoning=data['reasoning']
            )
        except Exception as e:
            logger.error(f"è§£æå“åº”å¤±è´¥: {e}")
            raise
    
    def _validate_result(
        self,
        result: LocationOutput,
        input: LocationInput
    ) -> LocationOutput:
        """éªŒè¯ç»“æœ"""
        
        # 1. æ£€æŸ¥åæ ‡æ˜¯å¦åœ¨å›¾ç‰‡èŒƒå›´å†…
        if (result.bbox.x < 0 or
            result.bbox.y < 0 or
            result.bbox.x + result.bbox.width > input.image_width or
            result.bbox.y + result.bbox.height > input.image_height):
            logger.warning("åæ ‡è¶…å‡ºå›¾ç‰‡èŒƒå›´ï¼Œé™ä½ç½®ä¿¡åº¦")
            result.confidence *= 0.5
        
        # 2. æ£€æŸ¥æ˜¯å¦åœ¨é¢˜ç›®åŒºåŸŸé™„è¿‘
        question_center_y = input.question_bbox.y + input.question_bbox.height / 2
        result_center_y = result.bbox.y + result.bbox.height / 2
        
        distance = abs(result_center_y - question_center_y)
        if distance > input.question_bbox.height:
            logger.warning("ä½ç½®è·ç¦»é¢˜ç›®åŒºåŸŸè¾ƒè¿œï¼Œé™ä½ç½®ä¿¡åº¦")
            result.confidence *= 0.7
        
        return result
    
    def _get_fallback_location(
        self,
        input: LocationInput
    ) -> LocationOutput:
        """å…œåº•æ–¹æ¡ˆ: è¿”å›é¢˜ç›®ä¸­å¿ƒä½ç½®"""
        center_x = input.question_bbox.x + input.question_bbox.width / 2
        center_y = input.question_bbox.y + input.question_bbox.height / 2
        
        return LocationOutput(
            bbox=BoundingBox(
                x=int(center_x - 50),
                y=int(center_y - 25),
                width=100,
                height=50
            ),
            type="area",
            confidence=0.3,
            reasoning="æ— æ³•å‡†ç¡®å®šä½ï¼Œè¿”å›é¢˜ç›®ä¸­å¿ƒä½ç½®"
        )
```

### æ€§èƒ½ä¼˜åŒ–

1. **æ‰¹é‡å¤„ç†**: ä¸€æ¬¡æ€§æ ‡æ³¨å¤šä¸ªé”™è¯¯
2. **ç¼“å­˜ç»“æœ**: ç›¸ä¼¼é”™è¯¯å¤ç”¨ä½ç½®
3. **å¹¶è¡Œè°ƒç”¨**: å¤šä¸ªé¢˜ç›®å¹¶è¡Œæ ‡æ³¨

---

## GradingOrchestrator

### å®Œæ•´å®ç°

```python
class GradingOrchestrator:
    def __init__(self):
        self.preprocess_agent = PreprocessAgent()
        self.grading_agent = UnifiedGradingAgent()
        self.location_agent = LocationAnnotationAgent()
        
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> CompiledGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(GradingState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("preprocess", self._preprocess_node)
        workflow.add_node("grade", self._grade_node)
        workflow.add_node("annotate", self._annotate_node)
        workflow.add_node("finalize", self._finalize_node)
        
        # å®šä¹‰æµç¨‹
        workflow.add_edge("__start__", "preprocess")
        workflow.add_edge("preprocess", "grade")
        workflow.add_edge("grade", "annotate")
        workflow.add_edge("annotate", "finalize")
        workflow.add_edge("finalize", "__end__")
        
        return workflow.compile()
    
    async def _preprocess_node(self, state: GradingState) -> GradingState:
        """é¢„å¤„ç†èŠ‚ç‚¹"""
        segments = await self.preprocess_agent.segment_questions(state["images"])
        return {**state, "question_segments": segments}
    
    async def _grade_node(self, state: GradingState) -> GradingState:
        """æ‰¹æ”¹èŠ‚ç‚¹"""
        results = await asyncio.gather(*[
            self.grading_agent.grade(segment)
            for segment in state["question_segments"]
        ])
        return {**state, "grading_results": results}
    
    async def _annotate_node(self, state: GradingState) -> GradingState:
        """æ ‡æ³¨èŠ‚ç‚¹"""
        annotated = []
        for question in state["grading_results"]:
            errors_with_location = await asyncio.gather(*[
                self.location_agent.annotate(LocationInput(
                    image_url=state["images"][question.page_index],
                    question_bbox=question.bbox,
                    error=error
                ))
                for error in question.errors
            ])
            annotated.append({**question, "errors": errors_with_location})
        
        return {**state, "annotated_results": annotated}
    
    async def _finalize_node(self, state: GradingState) -> GradingState:
        """æ±‡æ€»èŠ‚ç‚¹"""
        final_result = {
            "submission_id": state["submission_id"],
            "total_score": sum(q.score for q in state["annotated_results"]),
            "max_score": sum(q.max_score for q in state["annotated_results"]),
            "questions": state["annotated_results"]
        }
        return {**state, "final_result": final_result}
    
    async def grade(
        self,
        submission_id: str,
        images: List[str],
        progress_callback: Optional[Callable] = None
    ) -> GradingResult:
        """æ‰§è¡Œæ‰¹æ”¹"""
        initial_state = GradingState(
            submission_id=submission_id,
            images=images
        )
        
        result = await self.workflow.ainvoke(initial_state)
        
        return result["final_result"]
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2025-10-05  
**çŠ¶æ€**: âœ… å®Œæˆ

