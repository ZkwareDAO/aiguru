# Agentæ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ“Œ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°åŸºäºLangGraphçš„å¤šAgentåä½œæ¶æ„è®¾è®¡,åŒ…æ‹¬Agentç±»å‹ã€èŒè´£ã€åä½œæ¨¡å¼å’Œå®ç°ç»†èŠ‚ã€‚

---

## 1. æ¶æ„æ¦‚è§ˆ

### 1.1 è®¾è®¡ç†å¿µ

**æ ¸å¿ƒåŸåˆ™**:
- **å•ä¸€èŒè´£**: æ¯ä¸ªAgentä¸“æ³¨äºç‰¹å®šä»»åŠ¡
- **æ¾è€¦åˆ**: Agentä¹‹é—´é€šè¿‡æ¶ˆæ¯ä¼ é€’é€šä¿¡
- **å¯ç»„åˆ**: Agentå¯ä»¥çµæ´»ç»„åˆæˆä¸åŒçš„å·¥ä½œæµ
- **å¯è§‚æµ‹**: æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æ¸…æ™°çš„çŠ¶æ€å’Œæ—¥å¿—
- **å®¹é”™æ€§**: å•ä¸ªAgentå¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹

**æ¶æ„æ¨¡å¼**: **Supervisor Pattern + Pipeline Pattern**
```
Orchestrator (Supervisor)
    â”œâ”€â”€ PreprocessAgent (Pipeline Step 1)
    â”œâ”€â”€ GradingAgent (Pipeline Step 2)
    â”œâ”€â”€ ReviewAgent (Pipeline Step 3)
    â””â”€â”€ FeedbackAgent (Pipeline Step 4)
```

### 1.2 Agentå±‚æ¬¡ç»“æ„

```mermaid
graph TB
    User[ç”¨æˆ·è¯·æ±‚] --> Orchestrator[OrchestratorAgent<br/>ç¼–æ’å™¨]
    
    Orchestrator --> Preprocess[PreprocessAgent<br/>é¢„å¤„ç†]
    Orchestrator --> Grading[GradingAgent<br/>æ‰¹æ”¹]
    Orchestrator --> Review[ReviewAgent<br/>å®¡æ ¸]
    Orchestrator --> Feedback[FeedbackAgent<br/>åé¦ˆç”Ÿæˆ]
    Orchestrator --> Analytics[AnalyticsAgent<br/>åˆ†æ]
    
    Preprocess --> FileParser[æ–‡ä»¶è§£æ]
    Preprocess --> OCR[OCRè¯†åˆ«]
    Preprocess --> Validator[æ•°æ®éªŒè¯]
    
    Grading --> ScoreCalculator[åˆ†æ•°è®¡ç®—]
    Grading --> ErrorDetector[é”™è¯¯æ£€æµ‹]
    Grading --> AnnotationGenerator[æ ‡æ³¨ç”Ÿæˆ]
    
    Review --> QualityChecker[è´¨é‡æ£€æŸ¥]
    Review --> ConsistencyValidator[ä¸€è‡´æ€§éªŒè¯]
    
    Feedback --> SuggestionGenerator[å»ºè®®ç”Ÿæˆ]
    Feedback --> KnowledgeLinker[çŸ¥è¯†ç‚¹å…³è”]
    
    Analytics --> TrendAnalyzer[è¶‹åŠ¿åˆ†æ]
    Analytics --> WeakPointDetector[è–„å¼±ç‚¹è¯†åˆ«]
```

---

## 2. Agentè¯¦ç»†è®¾è®¡

### 2.1 OrchestratorAgent (ç¼–æ’å™¨)

**èŒè´£**: æ•´ä½“æµç¨‹ç¼–æ’ã€ä»»åŠ¡åˆ†å‘ã€çŠ¶æ€ç®¡ç†

**è¾“å…¥**:
```python
{
    "task_type": "single_grading" | "batch_grading",
    "submission_id": UUID,
    "assignment_id": UUID,
    "config": {
        "strictness": "loose" | "standard" | "strict",
        "enable_review": bool,
        "enable_analytics": bool
    }
}
```

**æ ¸å¿ƒé€»è¾‘**:
```python
class OrchestratorAgent:
    """ç¼–æ’å™¨Agent - è´Ÿè´£æ•´ä½“æµç¨‹æ§åˆ¶"""
    
    def __init__(self, llm, tools):
        self.llm = llm
        self.tools = tools
        self.state_graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(GradingState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("preprocess", self.preprocess_step)
        workflow.add_node("grade", self.grade_step)
        workflow.add_node("review", self.review_step)
        workflow.add_node("feedback", self.feedback_step)
        workflow.add_node("finalize", self.finalize_step)
        
        # å®šä¹‰è¾¹å’Œæ¡ä»¶
        workflow.set_entry_point("preprocess")
        workflow.add_edge("preprocess", "grade")
        workflow.add_conditional_edges(
            "grade",
            self.should_review,
            {
                "review": "review",
                "feedback": "feedback"
            }
        )
        workflow.add_edge("review", "feedback")
        workflow.add_edge("feedback", "finalize")
        workflow.set_finish_point("finalize")
        
        return workflow.compile()
    
    async def execute(self, input_data: Dict) -> Dict:
        """æ‰§è¡Œæ‰¹æ”¹æµç¨‹"""
        initial_state = GradingState(
            submission_id=input_data["submission_id"],
            status="pending",
            config=input_data["config"]
        )
        
        result = await self.state_graph.ainvoke(initial_state)
        return result
    
    def should_review(self, state: GradingState) -> str:
        """å†³å®šæ˜¯å¦éœ€è¦å®¡æ ¸"""
        if state.config.get("enable_review") and state.confidence < 0.8:
            return "review"
        return "feedback"
```

**çŠ¶æ€ç®¡ç†**:
```python
class GradingState(TypedDict):
    """æ‰¹æ”¹æµç¨‹çŠ¶æ€"""
    submission_id: UUID
    assignment_id: UUID
    status: str  # pending, preprocessing, grading, reviewing, completed, failed
    
    # é¢„å¤„ç†ç»“æœ
    preprocessed_files: List[Dict]
    extracted_text: str
    file_metadata: Dict
    
    # æ‰¹æ”¹ç»“æœ
    score: Optional[float]
    max_score: float
    errors: List[Dict]
    annotations: List[Dict]
    confidence: float
    
    # å®¡æ ¸ç»“æœ
    review_passed: bool
    review_comments: str
    adjusted_score: Optional[float]
    
    # åé¦ˆç»“æœ
    feedback_text: str
    suggestions: List[str]
    knowledge_points: List[str]
    
    # å…ƒæ•°æ®
    config: Dict
    created_at: datetime
    updated_at: datetime
    error_message: Optional[str]
```

**è¾“å‡º**:
```python
{
    "submission_id": UUID,
    "status": "completed",
    "result": {
        "score": 85.5,
        "max_score": 100,
        "feedback": "æ•´ä½“å®Œæˆè‰¯å¥½...",
        "errors": [...],
        "annotations": [...],
        "suggestions": [...]
    },
    "metadata": {
        "processing_time_ms": 2500,
        "confidence": 0.92,
        "review_passed": true
    }
}
```

---

### 2.2 PreprocessAgent (é¢„å¤„ç†å™¨)

**èŒè´£**: æ–‡ä»¶è§£æã€OCRè¯†åˆ«ã€æ•°æ®éªŒè¯ã€æ ¼å¼è½¬æ¢

**æ ¸å¿ƒåŠŸèƒ½**:
```python
class PreprocessAgent:
    """é¢„å¤„ç†Agent"""
    
    async def process(self, state: GradingState) -> GradingState:
        """é¢„å¤„ç†æµç¨‹"""
        # 1. è·å–æ–‡ä»¶
        files = await self._fetch_files(state.submission_id)
        
        # 2. æ–‡ä»¶ç±»å‹æ£€æµ‹
        file_types = [self._detect_file_type(f) for f in files]
        
        # 3. æ ¹æ®ç±»å‹å¤„ç†
        processed_files = []
        for file, file_type in zip(files, file_types):
            if file_type == "image":
                result = await self._process_image(file)
            elif file_type == "pdf":
                result = await self._process_pdf(file)
            elif file_type == "document":
                result = await self._process_document(file)
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
            
            processed_files.append(result)
        
        # 4. æå–æ–‡æœ¬
        extracted_text = self._extract_text(processed_files)
        
        # 5. æ•°æ®éªŒè¯
        validation_result = self._validate_data(extracted_text)
        if not validation_result.is_valid:
            state.status = "failed"
            state.error_message = validation_result.error
            return state
        
        # 6. æ›´æ–°çŠ¶æ€
        state.preprocessed_files = processed_files
        state.extracted_text = extracted_text
        state.status = "preprocessed"
        
        return state
    
    async def _process_image(self, file: File) -> Dict:
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶"""
        # OCRè¯†åˆ«
        ocr_result = await self.ocr_service.recognize(file.path)
        
        # å›¾åƒå¢å¼º
        enhanced_image = await self.image_enhancer.enhance(file.path)
        
        return {
            "file_id": file.id,
            "type": "image",
            "text": ocr_result.text,
            "confidence": ocr_result.confidence,
            "enhanced_path": enhanced_image.path,
            "metadata": {
                "width": enhanced_image.width,
                "height": enhanced_image.height,
                "format": enhanced_image.format
            }
        }
    
    async def _process_pdf(self, file: File) -> Dict:
        """å¤„ç†PDFæ–‡ä»¶"""
        # è½¬æ¢ä¸ºå›¾ç‰‡
        images = await self.pdf_converter.to_images(file.path)
        
        # æå–æ–‡æœ¬
        text = await self.pdf_parser.extract_text(file.path)
        
        return {
            "file_id": file.id,
            "type": "pdf",
            "text": text,
            "images": images,
            "page_count": len(images)
        }
```

**å·¥å…·é›†æˆ**:
- **OCR**: Tesseract / PaddleOCR / äº‘æœåŠ¡API
- **PDFå¤„ç†**: PyPDF2 / pdfplumber
- **å›¾åƒå¤„ç†**: Pillow / OpenCV
- **æ–‡æ¡£è§£æ**: python-docx / openpyxl

---

### 2.3 GradingAgent (æ‰¹æ”¹å™¨)

**èŒè´£**: æ ¸å¿ƒæ‰¹æ”¹é€»è¾‘ã€åˆ†æ•°è®¡ç®—ã€é”™è¯¯æ£€æµ‹ã€æ ‡æ³¨ç”Ÿæˆ

**æ ¸å¿ƒå®ç°**:
```python
class GradingAgent:
    """æ‰¹æ”¹Agent - æ ¸å¿ƒæ‰¹æ”¹é€»è¾‘"""
    
    def __init__(self, llm, grading_prompts):
        self.llm = llm
        self.prompts = grading_prompts
        self.error_detector = ErrorDetector()
        self.score_calculator = ScoreCalculator()
        self.annotation_generator = AnnotationGenerator()
    
    async def process(self, state: GradingState) -> GradingState:
        """æ‰§è¡Œæ‰¹æ”¹"""
        # 1. è·å–æ‰¹æ”¹æ ‡å‡†
        assignment = await self._get_assignment(state.assignment_id)
        grading_standard = assignment.grading_standard
        
        # 2. æ„å»ºæ‰¹æ”¹æç¤ºè¯
        prompt = self._build_grading_prompt(
            student_answer=state.extracted_text,
            grading_standard=grading_standard,
            config=state.config
        )
        
        # 3. è°ƒç”¨LLMè¿›è¡Œæ‰¹æ”¹
        grading_result = await self.llm.ainvoke(prompt)
        
        # 4. è§£ææ‰¹æ”¹ç»“æœ
        parsed_result = self._parse_grading_result(grading_result)
        
        # 5. é”™è¯¯æ£€æµ‹
        errors = await self.error_detector.detect(
            student_answer=state.extracted_text,
            standard_answer=grading_standard.answer,
            llm_errors=parsed_result.errors
        )
        
        # 6. åˆ†æ•°è®¡ç®—
        score = self.score_calculator.calculate(
            errors=errors,
            max_score=assignment.max_score,
            strictness=state.config.get("strictness", "standard")
        )
        
        # 7. ç”Ÿæˆæ ‡æ³¨
        annotations = await self.annotation_generator.generate(
            errors=errors,
            files=state.preprocessed_files
        )
        
        # 8. æ›´æ–°çŠ¶æ€
        state.score = score.value
        state.max_score = assignment.max_score
        state.errors = errors
        state.annotations = annotations
        state.confidence = parsed_result.confidence
        state.status = "graded"
        
        return state
    
    def _build_grading_prompt(self, student_answer, grading_standard, config):
        """æ„å»ºæ‰¹æ”¹æç¤ºè¯"""
        strictness = config.get("strictness", "standard")
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™å¸ˆ,æ­£åœ¨æ‰¹æ”¹å­¦ç”Ÿä½œä¸šã€‚

ã€æ‰¹æ”¹æ ‡å‡†ã€‘
{grading_standard.criteria}

ã€æ ‡å‡†ç­”æ¡ˆã€‘
{grading_standard.answer}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{student_answer}

ã€æ‰¹æ”¹è¦æ±‚ã€‘
- ä¸¥æ ¼ç¨‹åº¦: {strictness}
- è¯·é€é¡¹å¯¹ç…§æ ‡å‡†ç­”æ¡ˆ,æ‰¾å‡ºå­¦ç”Ÿç­”æ¡ˆä¸­çš„é”™è¯¯
- å¯¹æ¯ä¸ªé”™è¯¯,è¯·è¯´æ˜:
  1. é”™è¯¯ç±»å‹(æ¦‚å¿µé”™è¯¯/è®¡ç®—é”™è¯¯/è¡¨è¿°é”™è¯¯ç­‰)
  2. é”™è¯¯ä½ç½®(å…·ä½“åœ¨å“ªä¸€éƒ¨åˆ†)
  3. é”™è¯¯åŸå› 
  4. æ­£ç¡®ç­”æ¡ˆåº”è¯¥æ˜¯ä»€ä¹ˆ
- è¯·ç»™å‡ºæ€»ä½“è¯„ä»·å’Œæ”¹è¿›å»ºè®®

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»¥JSONæ ¼å¼è¾“å‡º,åŒ…å«ä»¥ä¸‹å­—æ®µ:
{{
    "score": åˆ†æ•°(0-100),
    "confidence": ç½®ä¿¡åº¦(0-1),
    "errors": [
        {{
            "type": "é”™è¯¯ç±»å‹",
            "location": "é”™è¯¯ä½ç½®æè¿°",
            "description": "é”™è¯¯è¯´æ˜",
            "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
            "severity": "high|medium|low"
        }}
    ],
    "overall_comment": "æ€»ä½“è¯„ä»·",
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "weaknesses": ["ä¸è¶³1", "ä¸è¶³2"]
}}
"""
        return prompt
```

**åˆ†æ•°è®¡ç®—ç­–ç•¥**:
```python
class ScoreCalculator:
    """åˆ†æ•°è®¡ç®—å™¨"""
    
    def calculate(self, errors, max_score, strictness):
        """è®¡ç®—åˆ†æ•°"""
        # åŸºç¡€åˆ†æ•°
        base_score = max_score
        
        # æ ¹æ®é”™è¯¯æ‰£åˆ†
        for error in errors:
            deduction = self._calculate_deduction(error, strictness)
            base_score -= deduction
        
        # ç¡®ä¿åˆ†æ•°åœ¨æœ‰æ•ˆèŒƒå›´å†…
        final_score = max(0, min(base_score, max_score))
        
        return Score(
            value=final_score,
            max_value=max_score,
            percentage=final_score / max_score * 100
        )
    
    def _calculate_deduction(self, error, strictness):
        """è®¡ç®—å•ä¸ªé”™è¯¯çš„æ‰£åˆ†"""
        # åŸºç¡€æ‰£åˆ†
        base_deduction = {
            "high": 10,
            "medium": 5,
            "low": 2
        }[error.severity]
        
        # ä¸¥æ ¼åº¦è°ƒæ•´
        strictness_multiplier = {
            "loose": 0.7,
            "standard": 1.0,
            "strict": 1.3
        }[strictness]
        
        return base_deduction * strictness_multiplier
```

---

### 2.4 ReviewAgent (å®¡æ ¸å™¨)

**èŒè´£**: è´¨é‡æ£€æŸ¥ã€ä¸€è‡´æ€§éªŒè¯ã€ç½®ä¿¡åº¦è¯„ä¼°

**å®ç°**:
```python
class ReviewAgent:
    """å®¡æ ¸Agent - è´¨é‡æ§åˆ¶"""
    
    async def process(self, state: GradingState) -> GradingState:
        """å®¡æ ¸æ‰¹æ”¹ç»“æœ"""
        # 1. ç½®ä¿¡åº¦æ£€æŸ¥
        if state.confidence < 0.6:
            state.review_passed = False
            state.review_comments = "æ‰¹æ”¹ç½®ä¿¡åº¦è¿‡ä½,å»ºè®®äººå·¥å¤æ ¸"
            return state
        
        # 2. ä¸€è‡´æ€§æ£€æŸ¥
        consistency_check = await self._check_consistency(state)
        if not consistency_check.passed:
            state.review_passed = False
            state.review_comments = consistency_check.reason
            return state
        
        # 3. åˆç†æ€§æ£€æŸ¥
        reasonableness_check = self._check_reasonableness(state)
        if not reasonableness_check.passed:
            # è°ƒæ•´åˆ†æ•°
            state.adjusted_score = reasonableness_check.suggested_score
            state.review_comments = reasonableness_check.reason
        
        # 4. é€šè¿‡å®¡æ ¸
        state.review_passed = True
        state.status = "reviewed"
        
        return state
    
    async def _check_consistency(self, state):
        """ä¸€è‡´æ€§æ£€æŸ¥"""
        # æ£€æŸ¥åˆ†æ•°ä¸é”™è¯¯æ•°é‡æ˜¯å¦åŒ¹é…
        expected_score = self._estimate_score_from_errors(state.errors)
        score_diff = abs(state.score - expected_score)
        
        if score_diff > 10:
            return ConsistencyResult(
                passed=False,
                reason=f"åˆ†æ•°({state.score})ä¸é”™è¯¯æ•°é‡ä¸åŒ¹é…,é¢„æœŸçº¦{expected_score}åˆ†"
            )
        
        return ConsistencyResult(passed=True)
```

---

## 3. Agentåä½œæ¨¡å¼

### 3.1 é€šä¿¡æœºåˆ¶

**æ¶ˆæ¯ä¼ é€’**:
```python
class AgentMessage(BaseModel):
    """Agenté—´æ¶ˆæ¯"""
    from_agent: str
    to_agent: str
    message_type: str  # request, response, notification
    payload: Dict[str, Any]
    timestamp: datetime
    correlation_id: UUID  # ç”¨äºè¿½è¸ªæ•´ä¸ªæµç¨‹
```

**çŠ¶æ€å…±äº«**:
- ä½¿ç”¨LangGraphçš„Stateæœºåˆ¶
- æ¯ä¸ªAgentè¯»å–å’Œæ›´æ–°å…±äº«çŠ¶æ€
- çŠ¶æ€å˜æ›´è‡ªåŠ¨è§¦å‘ä¸‹æ¸¸Agent

### 3.2 é”™è¯¯å¤„ç†

**é‡è¯•ç­–ç•¥**:
```python
class RetryConfig:
    max_retries: int = 3
    retry_delay: int = 5  # seconds
    backoff_multiplier: float = 2.0
    retry_on_errors: List[Type[Exception]] = [
        TimeoutError,
        ConnectionError,
        APIError
    ]
```

**é™çº§ç­–ç•¥**:
- ReviewAgentå¤±è´¥ â†’ è·³è¿‡å®¡æ ¸,ç›´æ¥ç”Ÿæˆåé¦ˆ
- AnalyticsAgentå¤±è´¥ â†’ è¿”å›åŸºç¡€æ‰¹æ”¹ç»“æœ
- æ•´ä½“å¤±è´¥ â†’ æ ‡è®°ä¸ºå¾…äººå·¥å¤„ç†

---

## 4. æ€§èƒ½ä¼˜åŒ–

### 4.1 å¹¶å‘å¤„ç†
```python
# æ‰¹é‡æ‰¹æ”¹æ—¶å¹¶å‘æ‰§è¡Œ
async def batch_grade(submission_ids):
    tasks = [
        orchestrator.execute({"submission_id": sid})
        for sid in submission_ids
    ]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    return results
```

### 4.2 ç¼“å­˜ç­–ç•¥
- **LLMå“åº”ç¼“å­˜**: ç›¸åŒè¾“å…¥ç¼“å­˜ç»“æœ
- **æ–‡ä»¶å¤„ç†ç¼“å­˜**: OCRç»“æœç¼“å­˜
- **æ‰¹æ”¹æ ‡å‡†ç¼“å­˜**: é¿å…é‡å¤æŸ¥è¯¢

---

## 5. æˆæœ¬ä¼˜åŒ–è€ƒè™‘ ğŸ’°

### 5.1 æˆæœ¬åˆ†æ

**é‡è¦æç¤º**: è™½ç„¶è®¾è®¡äº†5ä¸ªAgent,ä½†å®é™…ä¸Šåªæœ‰1-2ä¸ªAgentä¼šè°ƒç”¨LLM API:

| Agent | æ˜¯å¦è°ƒç”¨LLM | å•æ¬¡æˆæœ¬ | è¯´æ˜ |
|-------|------------|---------|------|
| OrchestratorAgent | âŒ | $0 | çº¯æµç¨‹æ§åˆ¶ |
| PreprocessAgent | âŒ | $0 | æ–‡ä»¶å¤„ç†ã€OCR |
| **GradingAgent** | âœ… | **$0.008-0.015** | ä¸»è¦æˆæœ¬æ¥æº |
| ReviewAgent | âŒ | $0 | è§„åˆ™éªŒè¯ã€é€»è¾‘æ£€æŸ¥ |
| **FeedbackAgent** | âš ï¸ å¯é€‰ | **$0.002-0.005** | å¯é€‰LLMè°ƒç”¨ |

**å•æ¬¡æ‰¹æ”¹æ€»æˆæœ¬**: $0.010 - $0.015 (çº¦Â¥0.07 - Â¥0.10)

**æœˆåº¦æˆæœ¬ä¼°ç®—** (å‡è®¾æ¯å¤©1000æ¬¡æ‰¹æ”¹):
- åŸºç¡€æ¨¡å¼: $0.008 Ã— 1000 Ã— 30 = **$240/æœˆ** (çº¦Â¥1,680/æœˆ)
- æ ‡å‡†æ¨¡å¼: $0.010 Ã— 1000 Ã— 30 = **$300/æœˆ** (çº¦Â¥2,100/æœˆ)
- å®Œæ•´æ¨¡å¼: $0.015 Ã— 1000 Ã— 30 = **$450/æœˆ** (çº¦Â¥3,150/æœˆ)

### 5.2 ä¼˜åŒ–æ–¹æ¡ˆ

#### æ–¹æ¡ˆ1: Agentèåˆ (æ¨è â­)

å°†GradingAgentå’ŒFeedbackAgentåˆå¹¶ä¸ºä¸€ä¸ªUnifiedAgent,ä¸€æ¬¡LLMè°ƒç”¨å®Œæˆæ‰€æœ‰ä»»åŠ¡:

```python
class UnifiedGradingAgent:
    """ç»Ÿä¸€æ‰¹æ”¹Agent - ä¸€æ¬¡LLMè°ƒç”¨å®Œæˆæ‰¹æ”¹+åé¦ˆ"""

    async def process(self, state: GradingState) -> GradingState:
        # æ„å»ºç»Ÿä¸€æç¤ºè¯,ä¸€æ¬¡æ€§å®Œæˆæ‰¹æ”¹å’Œåé¦ˆç”Ÿæˆ
        prompt = f"""
è¯·å®Œæˆä»¥ä¸‹æ‰¹æ”¹ä»»åŠ¡:

ã€æ‰¹æ”¹æ ‡å‡†ã€‘{state["grading_standard"]}
ã€å­¦ç”Ÿç­”æ¡ˆã€‘{state["extracted_text"]}

è¯·ä¸€æ¬¡æ€§å®Œæˆ:
1. æ‰¹æ”¹è¯„åˆ† - æ‰¾å‡ºé”™è¯¯,è®¡ç®—åˆ†æ•°
2. é”™è¯¯åˆ†æ - è¯¦ç»†è¯´æ˜æ¯ä¸ªé”™è¯¯
3. æ€»ä½“åé¦ˆ - ä¼˜ç‚¹ã€ä¸è¶³ã€æ”¹è¿›å»ºè®®
4. çŸ¥è¯†ç‚¹å…³è” - ç›¸å…³çŸ¥è¯†ç‚¹å’Œå­¦ä¹ å»ºè®®

è¾“å‡ºJSONæ ¼å¼:
{{
    "score": ...,
    "errors": [...],
    "feedback": "...",
    "suggestions": [...],
    "knowledge_points": [...]
}}
"""
        response = await self.llm.ainvoke(prompt)
        return self._parse_unified_result(response, state)
```

**æˆæœ¬å¯¹æ¯”**:
- åŸè®¾è®¡: GradingAgent($0.008) + FeedbackAgent($0.005) = $0.013
- ä¼˜åŒ–å: UnifiedAgent($0.010) = $0.010
- **èŠ‚çœ: 23%**

#### æ–¹æ¡ˆ2: æ™ºèƒ½æ¨¡å¼é€‰æ‹©

æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€é€‰æ‹©Agentç»„åˆ:

```python
class SmartOrchestrator:
    """æ™ºèƒ½ç¼–æ’å™¨ - æ ¹æ®åœºæ™¯é€‰æ‹©Agentç»„åˆ"""

    async def execute(self, input_data: Dict) -> Dict:
        # è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦
        complexity = await self._assess_complexity(input_data)

        if complexity == "simple":
            # å¿«é€Ÿæ¨¡å¼: ä½¿ç”¨UnifiedAgent (æˆæœ¬: $0.005)
            return await self.unified_agent.process(input_data)

        elif complexity == "medium":
            # æ ‡å‡†æ¨¡å¼: æ‰¹æ”¹ + è§„åˆ™å®¡æ ¸ (æˆæœ¬: $0.009)
            state = await self.grading_agent.process(input_data)
            state = await self.review_agent.process(state)  # ä¸è°ƒç”¨LLM
            return state

        else:  # complex
            # å®Œæ•´æ¨¡å¼: ä½¿ç”¨æ‰€æœ‰Agent (æˆæœ¬: $0.015)
            return await self.full_pipeline(input_data)

    async def _assess_complexity(self, input_data: Dict) -> str:
        """è¯„ä¼°ä»»åŠ¡å¤æ‚åº¦"""
        # ç®€å•ä»»åŠ¡: å•æ–‡ä»¶ã€å°å°ºå¯¸ã€åŸºç¡€é¢˜ç›®
        if (input_data["file_count"] == 1 and
            input_data["file_size"] < 1_000_000 and
            input_data["question_count"] <= 3):
            return "simple"

        # å¤æ‚ä»»åŠ¡: å¤šæ–‡ä»¶ã€å¤§å°ºå¯¸ã€å¤æ‚é¢˜ç›®
        elif (input_data["file_count"] > 3 or
              input_data["file_size"] > 5_000_000 or
              input_data["question_count"] > 10):
            return "complex"

        return "medium"
```

**æˆæœ¬èŠ‚çœ**: å¹³å‡ **40-50%**

#### æ–¹æ¡ˆ3: æ‰¹é‡å¤„ç†

å°†å¤šä»½ä½œä¸šåˆå¹¶åˆ°ä¸€æ¬¡LLMè°ƒç”¨:

```python
class BatchGradingAgent:
    """æ‰¹é‡æ‰¹æ”¹Agent"""

    async def batch_process(self, submissions: List[GradingState]) -> List[GradingState]:
        # æ„å»ºæ‰¹é‡æç¤ºè¯
        prompt = self._build_batch_prompt(submissions[:5])  # æ¯æ¬¡å¤„ç†5ä»½

        # ä¸€æ¬¡LLMè°ƒç”¨å¤„ç†å¤šä»½ä½œä¸š
        response = await self.llm.ainvoke(prompt)

        # è§£ææ‰¹é‡ç»“æœ
        return self._parse_batch_results(response)
```

**æˆæœ¬å¯¹æ¯”**:
- åŸè®¾è®¡: 5ä»½ä½œä¸š Ã— $0.010 = $0.050
- æ‰¹é‡å¤„ç†: 1æ¬¡è°ƒç”¨ = $0.020
- **èŠ‚çœ: 60%**

#### æ–¹æ¡ˆ4: æ™ºèƒ½ç¼“å­˜

ç¼“å­˜ç›¸ä¼¼é—®é¢˜çš„æ‰¹æ”¹ç»“æœ:

```python
@cache_similar(similarity_threshold=0.85)
async def grade_submission(submission_id: UUID):
    return await grading_agent.process(submission_id)
```

**æˆæœ¬èŠ‚çœ**: å‡è®¾ç¼“å­˜å‘½ä¸­ç‡30%,èŠ‚çœ **30%**

### 5.3 æ¨èå®æ–½ç­–ç•¥

**é˜¶æ®µ1: ç«‹å³å®æ–½** (èŠ‚çœ ~40%)
- âœ… å®ç°UnifiedGradingAgent
- âœ… æ·»åŠ æ™ºèƒ½æ¨¡å¼é€‰æ‹©
- âœ… æ·»åŠ ç›¸ä¼¼åº¦ç¼“å­˜

**é˜¶æ®µ2: ä¸­æœŸä¼˜åŒ–** (èŠ‚çœ ~60%)
- âšª å®ç°æ‰¹é‡å¤„ç†
- âšª ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹(ç®€å•ä»»åŠ¡ç”¨gpt-3.5-turbo)

**é˜¶æ®µ3: é•¿æœŸä¼˜åŒ–** (èŠ‚çœ ~80%)
- âšª è®­ç»ƒä¸“ç”¨æ¨¡å‹
- âšª æ··åˆä½¿ç”¨è‡ªæœ‰æ¨¡å‹å’ŒAPI

### 5.4 æˆæœ¬å¯¹æ¯”æ€»ç»“

| æ–¹æ¡ˆ | å•æ¬¡æˆæœ¬ | æœˆåº¦æˆæœ¬ | èŠ‚çœæ¯”ä¾‹ | å®æ–½éš¾åº¦ |
|------|---------|---------|---------|---------|
| åŸè®¾è®¡(å®Œæ•´) | $0.015 | $450 | 0% | - |
| Agentèåˆ | $0.010 | $300 | 33% | â­ ç®€å• |
| + æ™ºèƒ½æ¨¡å¼ | $0.009 | $270 | 40% | â­ ç®€å• |
| + æ‰¹é‡å¤„ç† | $0.006 | $180 | 60% | â­â­ ä¸­ç­‰ |
| + æ™ºèƒ½ç¼“å­˜ | $0.004 | $120 | 73% | â­â­ ä¸­ç­‰ |

**è¯¦ç»†çš„æˆæœ¬ä¼˜åŒ–ç­–ç•¥è¯·å‚è€ƒ**: [09_COST_OPTIMIZATION_STRATEGY.md](./09_COST_OPTIMIZATION_STRATEGY.md)

---

## 6. æ€»ç»“

æœ¬æ–‡æ¡£è¯¦ç»†è®¾è®¡äº†åŸºäºLangGraphçš„å¤šAgentåä½œæ¶æ„,åŒ…æ‹¬:

1. **æ¸…æ™°çš„Agentå±‚æ¬¡ç»“æ„**: OrchestratorAgentä½œä¸ºç¼–æ’å™¨,åè°ƒå…¶ä»–ä¸“ä¸šAgent
2. **å®Œå–„çš„çŠ¶æ€ç®¡ç†**: ä½¿ç”¨TypedDictå®šä¹‰çŠ¶æ€,ç¡®ä¿ç±»å‹å®‰å…¨
3. **çµæ´»çš„å·¥ä½œæµ**: æ”¯æŒæ¡ä»¶åˆ†æ”¯ã€å¹¶è¡Œå¤„ç†ç­‰å¤æ‚åœºæ™¯
4. **å¯æ‰©å±•çš„è®¾è®¡**: æ˜“äºæ·»åŠ æ–°çš„AgentèŠ‚ç‚¹
5. **æˆæœ¬å¯æ§**: å®é™…åªæœ‰1-2ä¸ªAgentè°ƒç”¨LLM,å•æ¬¡æˆæœ¬çº¦$0.010,é€šè¿‡ä¼˜åŒ–å¯é™ä½40-60%

**ä¸‹ä¸€æ­¥**:
- é˜…è¯» [03_COLLABORATION_STRATEGY.md](./03_COLLABORATION_STRATEGY.md) äº†è§£Agentåä½œç­–ç•¥
- é˜…è¯» [09_COST_OPTIMIZATION_STRATEGY.md](./09_COST_OPTIMIZATION_STRATEGY.md) äº†è§£è¯¦ç»†çš„æˆæœ¬ä¼˜åŒ–æ–¹æ¡ˆ
- é˜…è¯» [07_IMPLEMENTATION_GUIDE.md](./07_IMPLEMENTATION_GUIDE.md) å¼€å§‹å®ç°

---

**æ–‡æ¡£å®Œæˆ!** ğŸ‰
