# å¯æ‰©å±•æ€§ä¸å¯é æ€§è®¾è®¡æ–‡æ¡£

## ğŸ“Œ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†æè¿°ç³»ç»Ÿçš„å¯æ‰©å±•æ€§è®¾è®¡ã€å¯é æ€§ä¿éšœã€é«˜å¯ç”¨æ¶æ„å’Œæ€§èƒ½ä¼˜åŒ–ç­–ç•¥ã€‚

---

## 1. å¯æ‰©å±•æ€§è®¾è®¡

### 1.1 æ°´å¹³æ‰©å±•æ¶æ„

```mermaid
graph TB
    LB[è´Ÿè½½å‡è¡¡å™¨<br/>Nginx/HAProxy]
    
    subgraph "APIå±‚ - å¯æ°´å¹³æ‰©å±•"
        API1[FastAPIå®ä¾‹1]
        API2[FastAPIå®ä¾‹2]
        API3[FastAPIå®ä¾‹N]
    end
    
    subgraph "Workerå±‚ - å¯æ°´å¹³æ‰©å±•"
        W1[Worker 1<br/>æ‰¹æ”¹ä»»åŠ¡]
        W2[Worker 2<br/>æ‰¹æ”¹ä»»åŠ¡]
        W3[Worker N<br/>æ‰¹æ”¹ä»»åŠ¡]
    end
    
    subgraph "å­˜å‚¨å±‚"
        PG[(PostgreSQL<br/>ä¸»ä»å¤åˆ¶)]
        Redis[(Redis<br/>é›†ç¾¤æ¨¡å¼)]
        S3[(å¯¹è±¡å­˜å‚¨<br/>S3/MinIO)]
    end
    
    LB --> API1
    LB --> API2
    LB --> API3
    
    API1 --> Redis
    API2 --> Redis
    API3 --> Redis
    
    Redis --> W1
    Redis --> W2
    Redis --> W3
    
    W1 --> PG
    W2 --> PG
    W3 --> PG
    
    W1 --> S3
    W2 --> S3
    W3 --> S3
```

### 1.2 ä»»åŠ¡é˜Ÿåˆ—æ‰©å±•

**åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—**:
```python
class DistributedTaskQueue:
    """åˆ†å¸ƒå¼ä»»åŠ¡é˜Ÿåˆ—"""
    
    def __init__(self, redis_cluster):
        self.redis = redis_cluster
        self.queue_prefix = "task_queue"
        self.worker_id = f"worker-{uuid4()}"
    
    async def enqueue(
        self,
        task: TaskDefinition,
        queue_name: str = "default"
    ) -> UUID:
        """å…¥é˜Ÿä»»åŠ¡"""
        queue_key = f"{self.queue_prefix}:{queue_name}"
        
        # ä½¿ç”¨Redis Sorted Setå®ç°ä¼˜å…ˆçº§é˜Ÿåˆ—
        await self.redis.zadd(
            queue_key,
            {task.json(): task.priority.value}
        )
        
        # å‘å¸ƒä»»åŠ¡é€šçŸ¥
        await self.redis.publish(
            f"task_notification:{queue_name}",
            task.id.hex
        )
        
        return task.id
    
    async def dequeue(
        self,
        queue_name: str = "default",
        timeout: int = 30
    ) -> Optional[TaskDefinition]:
        """å‡ºé˜Ÿä»»åŠ¡(é˜»å¡å¼)"""
        queue_key = f"{self.queue_prefix}:{queue_name}"
        
        # ä½¿ç”¨ZPOPMAXè·å–æœ€é«˜ä¼˜å…ˆçº§ä»»åŠ¡
        result = await self.redis.zpopmax(queue_key)
        
        if result:
            task_json, priority = result[0]
            task = TaskDefinition(**json.loads(task_json))
            
            # æ ‡è®°ä¸ºå¤„ç†ä¸­
            await self._mark_processing(task)
            
            return task
        
        return None
    
    async def _mark_processing(self, task: TaskDefinition):
        """æ ‡è®°ä»»åŠ¡ä¸ºå¤„ç†ä¸­"""
        processing_key = f"processing:{self.worker_id}:{task.id}"
        await self.redis.setex(
            processing_key,
            task.timeout,
            task.json()
        )
    
    async def complete_task(self, task_id: UUID, result: Dict):
        """å®Œæˆä»»åŠ¡"""
        processing_key = f"processing:{self.worker_id}:{task_id}"
        await self.redis.delete(processing_key)
        
        # å­˜å‚¨ç»“æœ
        result_key = f"result:{task_id}"
        await self.redis.setex(
            result_key,
            86400,  # 24å°æ—¶
            json.dumps(result, default=str)
        )
```

**åŠ¨æ€Workeræ‰©å±•**:
```python
class WorkerPool:
    """Workeræ±  - æ”¯æŒåŠ¨æ€æ‰©ç¼©å®¹"""
    
    def __init__(
        self,
        min_workers: int = 3,
        max_workers: int = 20,
        scale_up_threshold: float = 0.8,
        scale_down_threshold: float = 0.3
    ):
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.scale_up_threshold = scale_up_threshold
        self.scale_down_threshold = scale_down_threshold
        
        self.workers: List[asyncio.Task] = []
        self.task_queue = DistributedTaskQueue()
        self.running = False
    
    async def start(self):
        """å¯åŠ¨Workeræ± """
        self.running = True
        
        # å¯åŠ¨æœ€å°æ•°é‡çš„Worker
        for i in range(self.min_workers):
            await self._spawn_worker(f"worker-{i}")
        
        # å¯åŠ¨è‡ªåŠ¨æ‰©ç¼©å®¹ç›‘æ§
        asyncio.create_task(self._auto_scale())
    
    async def _spawn_worker(self, worker_id: str):
        """åˆ›å»ºWorker"""
        worker = asyncio.create_task(self._worker_loop(worker_id))
        self.workers.append(worker)
        logger.info(f"Spawned worker: {worker_id}")
    
    async def _worker_loop(self, worker_id: str):
        """Workerä¸»å¾ªç¯"""
        while self.running:
            try:
                # è·å–ä»»åŠ¡
                task = await self.task_queue.dequeue(timeout=30)
                
                if task:
                    # å¤„ç†ä»»åŠ¡
                    result = await self._process_task(task)
                    await self.task_queue.complete_task(task.id, result)
                
            except Exception as e:
                logger.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(5)
    
    async def _auto_scale(self):
        """è‡ªåŠ¨æ‰©ç¼©å®¹"""
        while self.running:
            try:
                # è·å–é˜Ÿåˆ—é•¿åº¦å’ŒWorkerè´Ÿè½½
                queue_length = await self.task_queue.get_queue_length()
                active_workers = len([w for w in self.workers if not w.done()])
                
                # è®¡ç®—è´Ÿè½½ç‡
                if active_workers > 0:
                    load_ratio = queue_length / active_workers
                else:
                    load_ratio = float('inf')
                
                # æ‰©å®¹
                if load_ratio > self.scale_up_threshold and active_workers < self.max_workers:
                    new_worker_id = f"worker-{len(self.workers)}"
                    await self._spawn_worker(new_worker_id)
                    logger.info(f"Scaled up: {active_workers} -> {active_workers + 1}")
                
                # ç¼©å®¹
                elif load_ratio < self.scale_down_threshold and active_workers > self.min_workers:
                    # åœæ­¢ä¸€ä¸ªWorker
                    worker = self.workers.pop()
                    worker.cancel()
                    logger.info(f"Scaled down: {active_workers} -> {active_workers - 1}")
                
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                logger.error(f"Auto-scale error: {e}")
                await asyncio.sleep(60)
```

### 1.3 æ•°æ®åº“æ‰©å±•

**è¯»å†™åˆ†ç¦»**:
```python
class DatabaseRouter:
    """æ•°æ®åº“è·¯ç”± - è¯»å†™åˆ†ç¦»"""
    
    def __init__(self):
        self.master_url = settings.DATABASE_MASTER_URL
        self.replica_urls = settings.DATABASE_REPLICA_URLS
        self.current_replica_index = 0
    
    def get_write_engine(self):
        """è·å–å†™åº“è¿æ¥"""
        return create_async_engine(self.master_url)
    
    def get_read_engine(self):
        """è·å–è¯»åº“è¿æ¥(è½®è¯¢)"""
        if not self.replica_urls:
            return self.get_write_engine()
        
        # è½®è¯¢é€‰æ‹©ä»åº“
        replica_url = self.replica_urls[self.current_replica_index]
        self.current_replica_index = (self.current_replica_index + 1) % len(self.replica_urls)
        
        return create_async_engine(replica_url)

# ä½¿ç”¨
class GradingService:
    async def create_task(self, task_data: GradingTaskCreate):
        """åˆ›å»ºä»»åŠ¡(å†™æ“ä½œ)"""
        async with get_db_session(write=True) as db:
            task = GradingTask(**task_data.dict())
            db.add(task)
            await db.commit()
            return task
    
    async def get_task(self, task_id: UUID):
        """è·å–ä»»åŠ¡(è¯»æ“ä½œ)"""
        async with get_db_session(write=False) as db:
            result = await db.execute(
                select(GradingTask).where(GradingTask.id == task_id)
            )
            return result.scalar_one_or_none()
```

**åˆ†ç‰‡ç­–ç•¥**:
```python
class ShardingStrategy:
    """åˆ†ç‰‡ç­–ç•¥"""
    
    @staticmethod
    def get_shard_key(user_id: UUID) -> int:
        """æ ¹æ®ç”¨æˆ·IDè®¡ç®—åˆ†ç‰‡é”®"""
        # ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œ
        return int(hashlib.md5(user_id.bytes).hexdigest(), 16) % 10
    
    @staticmethod
    def get_database_url(shard_key: int) -> str:
        """æ ¹æ®åˆ†ç‰‡é”®è·å–æ•°æ®åº“URL"""
        shard_urls = settings.DATABASE_SHARD_URLS
        return shard_urls[shard_key]
```

---

## 2. å¯é æ€§ä¿éšœ

### 2.1 æ•°æ®ä¸€è‡´æ€§

**åˆ†å¸ƒå¼é”**:
```python
class DistributedLock:
    """åŸºäºRedisçš„åˆ†å¸ƒå¼é”"""
    
    def __init__(self, redis_client, key: str, timeout: int = 30):
        self.redis = redis_client
        self.key = f"lock:{key}"
        self.timeout = timeout
        self.lock_id = str(uuid4())
    
    async def __aenter__(self):
        """è·å–é”"""
        while True:
            # å°è¯•è·å–é”
            acquired = await self.redis.set(
                self.key,
                self.lock_id,
                nx=True,  # åªåœ¨é”®ä¸å­˜åœ¨æ—¶è®¾ç½®
                ex=self.timeout
            )
            
            if acquired:
                return self
            
            # ç­‰å¾…åé‡è¯•
            await asyncio.sleep(0.1)
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é‡Šæ”¾é”"""
        # ä½¿ç”¨Luaè„šæœ¬ç¡®ä¿åŸå­æ€§
        lua_script = """
        if redis.call("get", KEYS[1]) == ARGV[1] then
            return redis.call("del", KEYS[1])
        else
            return 0
        end
        """
        await self.redis.eval(lua_script, 1, self.key, self.lock_id)

# ä½¿ç”¨
async def update_submission_score(submission_id: UUID, score: float):
    """æ›´æ–°æäº¤åˆ†æ•°(é˜²æ­¢å¹¶å‘å†²çª)"""
    async with DistributedLock(redis_client, f"submission:{submission_id}"):
        # è¯»å–å½“å‰æ•°æ®
        submission = await get_submission(submission_id)
        
        # æ›´æ–°åˆ†æ•°
        submission.score = score
        submission.updated_at = datetime.utcnow()
        
        # ä¿å­˜
        await save_submission(submission)
```

**å¹‚ç­‰æ€§è®¾è®¡**:
```python
class IdempotencyManager:
    """å¹‚ç­‰æ€§ç®¡ç†å™¨"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def is_processed(self, idempotency_key: str) -> bool:
        """æ£€æŸ¥è¯·æ±‚æ˜¯å¦å·²å¤„ç†"""
        key = f"idempotency:{idempotency_key}"
        return await self.redis.exists(key)
    
    async def mark_processed(
        self,
        idempotency_key: str,
        result: Dict,
        ttl: int = 86400
    ):
        """æ ‡è®°è¯·æ±‚å·²å¤„ç†"""
        key = f"idempotency:{idempotency_key}"
        await self.redis.setex(
            key,
            ttl,
            json.dumps(result, default=str)
        )
    
    async def get_result(self, idempotency_key: str) -> Optional[Dict]:
        """è·å–å·²å¤„ç†è¯·æ±‚çš„ç»“æœ"""
        key = f"idempotency:{idempotency_key}"
        data = await self.redis.get(key)
        return json.loads(data) if data else None

# ä½¿ç”¨
@app.post("/api/v1/grading/submit")
async def submit_grading(
    request: GradingRequest,
    idempotency_key: str = Header(None)
):
    """æäº¤æ‰¹æ”¹è¯·æ±‚(å¹‚ç­‰)"""
    if not idempotency_key:
        idempotency_key = str(uuid4())
    
    # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
    if await idempotency_manager.is_processed(idempotency_key):
        result = await idempotency_manager.get_result(idempotency_key)
        return result
    
    # å¤„ç†è¯·æ±‚
    result = await grading_service.submit(request)
    
    # æ ‡è®°å·²å¤„ç†
    await idempotency_manager.mark_processed(idempotency_key, result)
    
    return result
```

### 2.2 æ•…éšœæ¢å¤

**å¥åº·æ£€æŸ¥**:
```python
class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    async def check_database(self) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥"""
        try:
            async with get_db_session() as db:
                await db.execute(text("SELECT 1"))
            return True
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False
    
    async def check_redis(self) -> bool:
        """æ£€æŸ¥Redisè¿æ¥"""
        try:
            await redis_client.ping()
            return True
        except Exception as e:
            logger.error(f"Redis health check failed: {e}")
            return False
    
    async def check_ai_service(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡"""
        try:
            # å‘é€æµ‹è¯•è¯·æ±‚
            response = await ai_service.health_check()
            return response.status == "ok"
        except Exception as e:
            logger.error(f"AI service health check failed: {e}")
            return False
    
    async def get_health_status(self) -> Dict:
        """è·å–æ•´ä½“å¥åº·çŠ¶æ€"""
        checks = await asyncio.gather(
            self.check_database(),
            self.check_redis(),
            self.check_ai_service(),
            return_exceptions=True
        )
        
        return {
            "status": "healthy" if all(checks) else "unhealthy",
            "checks": {
                "database": checks[0],
                "redis": checks[1],
                "ai_service": checks[2]
            },
            "timestamp": datetime.utcnow().isoformat()
        }

# APIç«¯ç‚¹
@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    health_checker = HealthChecker()
    status = await health_checker.get_health_status()
    
    status_code = 200 if status["status"] == "healthy" else 503
    return JSONResponse(content=status, status_code=status_code)
```

**è‡ªåŠ¨æ¢å¤**:
```python
class AutoRecovery:
    """è‡ªåŠ¨æ¢å¤æœºåˆ¶"""
    
    async def recover_failed_tasks(self):
        """æ¢å¤å¤±è´¥çš„ä»»åŠ¡"""
        # æŸ¥æ‰¾å¤±è´¥çš„ä»»åŠ¡
        async with get_db_session() as db:
            result = await db.execute(
                select(GradingTask)
                .where(GradingTask.status == GradingTaskStatus.FAILED)
                .where(GradingTask.retry_count < 3)
            )
            failed_tasks = result.scalars().all()
        
        # é‡æ–°å…¥é˜Ÿ
        for task in failed_tasks:
            await task_queue.enqueue_task(
                task_name="grading_task",
                payload={"task_id": str(task.id)},
                priority=TaskPriority.HIGH
            )
            
            # æ›´æ–°é‡è¯•æ¬¡æ•°
            task.retry_count += 1
            task.status = GradingTaskStatus.PENDING
            await db.commit()
        
        logger.info(f"Recovered {len(failed_tasks)} failed tasks")
    
    async def recover_stuck_tasks(self):
        """æ¢å¤å¡ä½çš„ä»»åŠ¡"""
        # æŸ¥æ‰¾é•¿æ—¶é—´å¤„ç†ä¸­çš„ä»»åŠ¡
        timeout_threshold = datetime.utcnow() - timedelta(minutes=30)
        
        async with get_db_session() as db:
            result = await db.execute(
                select(GradingTask)
                .where(GradingTask.status == GradingTaskStatus.PROCESSING)
                .where(GradingTask.updated_at < timeout_threshold)
            )
            stuck_tasks = result.scalars().all()
        
        # é‡ç½®çŠ¶æ€å¹¶é‡æ–°å…¥é˜Ÿ
        for task in stuck_tasks:
            task.status = GradingTaskStatus.PENDING
            await db.commit()
            
            await task_queue.enqueue_task(
                task_name="grading_task",
                payload={"task_id": str(task.id)},
                priority=TaskPriority.HIGH
            )
        
        logger.info(f"Recovered {len(stuck_tasks)} stuck tasks")
```

---

## 3. é«˜å¯ç”¨æ¶æ„

### 3.1 æœåŠ¡é™çº§

**é™çº§ç­–ç•¥**:
```python
class CircuitBreaker:
    """ç†”æ–­å™¨"""
    
    def __init__(
        self,
        failure_threshold: int = 5,
        recovery_timeout: int = 60,
        expected_exception: Type[Exception] = Exception
    ):
        self.failure_threshold = failure_threshold
        self.recovery_timeout = recovery_timeout
        self.expected_exception = expected_exception
        
        self.failure_count = 0
        self.last_failure_time = None
        self.state = "closed"  # closed, open, half_open
    
    async def call(self, func: Callable, *args, **kwargs):
        """è°ƒç”¨å‡½æ•°(å¸¦ç†”æ–­)"""
        if self.state == "open":
            # æ£€æŸ¥æ˜¯å¦å¯ä»¥å°è¯•æ¢å¤
            if time.time() - self.last_failure_time > self.recovery_timeout:
                self.state = "half_open"
            else:
                raise CircuitBreakerOpenError("Circuit breaker is open")
        
        try:
            result = await func(*args, **kwargs)
            
            # æˆåŠŸè°ƒç”¨,é‡ç½®è®¡æ•°
            if self.state == "half_open":
                self.state = "closed"
                self.failure_count = 0
            
            return result
            
        except self.expected_exception as e:
            self.failure_count += 1
            self.last_failure_time = time.time()
            
            if self.failure_count >= self.failure_threshold:
                self.state = "open"
                logger.warning(f"Circuit breaker opened after {self.failure_count} failures")
            
            raise

# ä½¿ç”¨
class AIGradingService:
    def __init__(self):
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60
        )
    
    async def grade_with_ai(self, submission_id: UUID):
        """AIæ‰¹æ”¹(å¸¦ç†”æ–­)"""
        try:
            return await self.circuit_breaker.call(
                self._call_ai_api,
                submission_id
            )
        except CircuitBreakerOpenError:
            # é™çº§åˆ°ç®€å•æ‰¹æ”¹
            logger.warning("AI service unavailable, using fallback")
            return await self._simple_grading(submission_id)
```

### 3.2 é™æµä¿æŠ¤

**å¤šçº§é™æµ**:
```python
class RateLimiter:
    """å¤šçº§é™æµå™¨"""
    
    def __init__(self, redis_client):
        self.redis = redis_client
    
    async def check_rate_limit(
        self,
        key: str,
        limit: int,
        window: int
    ) -> Tuple[bool, Dict]:
        """æ£€æŸ¥é™æµ"""
        current_time = int(time.time())
        window_key = f"rate_limit:{key}:{current_time // window}"
        
        # ä½¿ç”¨Luaè„šæœ¬å®ç°åŸå­æ“ä½œ
        lua_script = """
        local current = redis.call('INCR', KEYS[1])
        if current == 1 then
            redis.call('EXPIRE', KEYS[1], ARGV[1])
        end
        return current
        """
        
        current_count = await self.redis.eval(
            lua_script,
            1,
            window_key,
            window
        )
        
        allowed = current_count <= limit
        remaining = max(0, limit - current_count)
        
        return allowed, {
            "limit": limit,
            "remaining": remaining,
            "reset": (current_time // window + 1) * window
        }

# ä¸­é—´ä»¶
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """é™æµä¸­é—´ä»¶"""
    # ç”¨æˆ·çº§é™æµ
    user_id = request.state.user_id if hasattr(request.state, "user_id") else "anonymous"
    allowed, info = await rate_limiter.check_rate_limit(
        key=f"user:{user_id}",
        limit=100,  # æ¯åˆ†é’Ÿ100æ¬¡
        window=60
    )
    
    if not allowed:
        return JSONResponse(
            status_code=429,
            content={"error": "Rate limit exceeded"},
            headers={
                "X-RateLimit-Limit": str(info["limit"]),
                "X-RateLimit-Remaining": str(info["remaining"]),
                "X-RateLimit-Reset": str(info["reset"])
            }
        )
    
    response = await call_next(request)
    
    # æ·»åŠ é™æµå¤´
    response.headers["X-RateLimit-Limit"] = str(info["limit"])
    response.headers["X-RateLimit-Remaining"] = str(info["remaining"])
    response.headers["X-RateLimit-Reset"] = str(info["reset"])
    
    return response
```

---

## 4. æ€§èƒ½ä¼˜åŒ–

### 4.1 ç¼“å­˜ç­–ç•¥

**å¤šçº§ç¼“å­˜**:
```python
class MultiLevelCache:
    """å¤šçº§ç¼“å­˜"""
    
    def __init__(self):
        self.l1_cache = {}  # å†…å­˜ç¼“å­˜
        self.l1_max_size = 1000
        self.redis = redis_client  # Redisç¼“å­˜
    
    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜"""
        # L1ç¼“å­˜
        if key in self.l1_cache:
            return self.l1_cache[key]
        
        # L2ç¼“å­˜(Redis)
        data = await self.redis.get(f"cache:{key}")
        if data:
            value = json.loads(data)
            # å›å¡«L1ç¼“å­˜
            self._set_l1(key, value)
            return value
        
        return None
    
    async def set(
        self,
        key: str,
        value: Any,
        ttl: int = 3600
    ):
        """è®¾ç½®ç¼“å­˜"""
        # L1ç¼“å­˜
        self._set_l1(key, value)
        
        # L2ç¼“å­˜
        await self.redis.setex(
            f"cache:{key}",
            ttl,
            json.dumps(value, default=str)
        )
    
    def _set_l1(self, key: str, value: Any):
        """è®¾ç½®L1ç¼“å­˜"""
        if len(self.l1_cache) >= self.l1_max_size:
            # LRUæ·˜æ±°
            self.l1_cache.pop(next(iter(self.l1_cache)))
        self.l1_cache[key] = value
```

---

**ä¸‹ä¸€æ­¥**: æŸ¥çœ‹ `05_USER_EXPERIENCE_DESIGN.md` äº†è§£ç”¨æˆ·ä½“éªŒè®¾è®¡

