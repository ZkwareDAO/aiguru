# å®ç°æŒ‡å—

## ğŸ“Œ æ–‡æ¡£æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›åŸºäºè®¾è®¡æ–‡æ¡£çš„å…·ä½“å®ç°æŒ‡å—,åŒ…æ‹¬ä»£ç ç¤ºä¾‹ã€æœ€ä½³å®è·µå’Œå¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆã€‚

---

## 1. LangGraph Agentå®ç°

### 1.1 ç¯å¢ƒå‡†å¤‡

```bash
# å®‰è£…ä¾èµ–
pip install langchain langgraph langchain-openai
pip install fastapi uvicorn sqlalchemy asyncpg redis

# æˆ–ä½¿ç”¨é¡¹ç›®çš„requirements.txt
pip install -r backend/requirements.txt
```

### 1.2 åŸºç¡€Agentå®ç°

**Step 1: å®šä¹‰çŠ¶æ€**

```python
# backend/app/agents/state.py
from typing import TypedDict, List, Dict, Optional, Annotated
from datetime import datetime
from uuid import UUID
from langgraph.graph import add_messages

class GradingState(TypedDict):
    """æ‰¹æ”¹æµç¨‹çš„å…±äº«çŠ¶æ€"""
    
    # åŸºç¡€ä¿¡æ¯
    submission_id: UUID
    assignment_id: UUID
    student_id: UUID
    
    # å¤„ç†çŠ¶æ€
    status: str  # pending, preprocessing, grading, reviewing, completed, failed
    current_step: str
    progress: float  # 0-100
    
    # æ¶ˆæ¯å†å²
    messages: Annotated[List[Dict], add_messages]
    
    # é¢„å¤„ç†ç»“æœ
    preprocessed_files: Optional[List[Dict]]
    extracted_text: Optional[str]
    file_metadata: Optional[Dict]
    
    # æ‰¹æ”¹ç»“æœ
    score: Optional[float]
    max_score: float
    errors: Optional[List[Dict]]
    annotations: Optional[List[Dict]]
    confidence: Optional[float]
    
    # å®¡æ ¸ç»“æœ
    review_passed: Optional[bool]
    review_comments: Optional[str]
    adjusted_score: Optional[float]
    
    # åé¦ˆç»“æœ
    feedback_text: Optional[str]
    suggestions: Optional[List[str]]
    knowledge_points: Optional[List[str]]
    
    # é…ç½®
    config: Dict
    
    # å…ƒæ•°æ®
    created_at: datetime
    updated_at: datetime
    error_message: Optional[str]
```

**Step 2: å®ç°PreprocessAgent**

```python
# backend/app/agents/preprocess_agent.py
import logging
from typing import Dict, List
from pathlib import Path

from app.agents.state import GradingState
from app.services.file_service import FileService
from app.services.ocr_service import OCRService

logger = logging.getLogger(__name__)


class PreprocessAgent:
    """é¢„å¤„ç†Agent - è´Ÿè´£æ–‡ä»¶è§£æã€OCRè¯†åˆ«ã€æ•°æ®éªŒè¯"""
    
    def __init__(self):
        self.file_service = FileService()
        self.ocr_service = OCRService()
        self.name = "PreprocessAgent"
    
    async def process(self, state: GradingState) -> GradingState:
        """æ‰§è¡Œé¢„å¤„ç†æµç¨‹"""
        logger.info(f"[{self.name}] Starting preprocessing for submission {state['submission_id']}")
        
        try:
            # æ›´æ–°çŠ¶æ€
            state["status"] = "preprocessing"
            state["current_step"] = "preprocess"
            state["messages"].append({
                "agent": self.name,
                "action": "started",
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # 1. è·å–æ–‡ä»¶
            files = await self.file_service.get_submission_files(state["submission_id"])
            logger.info(f"[{self.name}] Found {len(files)} files")
            
            # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶
            preprocessed_files = []
            all_text = []
            
            for file in files:
                file_result = await self._process_file(file)
                preprocessed_files.append(file_result)
                
                if file_result.get("text"):
                    all_text.append(file_result["text"])
            
            # 3. åˆå¹¶æ–‡æœ¬
            extracted_text = "\n\n".join(all_text)
            
            # 4. æ•°æ®éªŒè¯
            validation_result = self._validate_data(extracted_text, preprocessed_files)
            
            if not validation_result["is_valid"]:
                state["status"] = "failed"
                state["error_message"] = validation_result["error"]
                logger.error(f"[{self.name}] Validation failed: {validation_result['error']}")
                return state
            
            # 5. æ›´æ–°çŠ¶æ€
            state["preprocessed_files"] = preprocessed_files
            state["extracted_text"] = extracted_text
            state["file_metadata"] = {
                "file_count": len(files),
                "total_text_length": len(extracted_text),
                "file_types": [f["type"] for f in preprocessed_files]
            }
            state["status"] = "preprocessed"
            state["progress"] = 25.0
            
            state["messages"].append({
                "agent": self.name,
                "action": "completed",
                "timestamp": datetime.utcnow().isoformat(),
                "result": {
                    "files_processed": len(preprocessed_files),
                    "text_length": len(extracted_text)
                }
            })
            
            logger.info(f"[{self.name}] Preprocessing completed successfully")
            return state
            
        except Exception as e:
            logger.error(f"[{self.name}] Error during preprocessing: {e}", exc_info=True)
            state["status"] = "failed"
            state["error_message"] = str(e)
            return state
    
    async def _process_file(self, file: Dict) -> Dict:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        file_type = file["type"]
        
        if file_type.startswith("image/"):
            return await self._process_image(file)
        elif file_type == "application/pdf":
            return await self._process_pdf(file)
        else:
            raise ValueError(f"Unsupported file type: {file_type}")
    
    async def _process_image(self, file: Dict) -> Dict:
        """å¤„ç†å›¾ç‰‡æ–‡ä»¶"""
        # OCRè¯†åˆ«
        ocr_result = await self.ocr_service.recognize(file["path"])
        
        return {
            "file_id": file["id"],
            "type": "image",
            "path": file["path"],
            "text": ocr_result["text"],
            "confidence": ocr_result["confidence"],
            "metadata": {
                "width": ocr_result.get("width"),
                "height": ocr_result.get("height")
            }
        }
    
    async def _process_pdf(self, file: Dict) -> Dict:
        """å¤„ç†PDFæ–‡ä»¶"""
        # æå–æ–‡æœ¬
        text = await self.file_service.extract_pdf_text(file["path"])
        
        return {
            "file_id": file["id"],
            "type": "pdf",
            "path": file["path"],
            "text": text,
            "metadata": {
                "page_count": await self.file_service.get_pdf_page_count(file["path"])
            }
        }
    
    def _validate_data(self, text: str, files: List[Dict]) -> Dict:
        """éªŒè¯æ•°æ®"""
        if not text or len(text.strip()) < 10:
            return {
                "is_valid": False,
                "error": "æå–çš„æ–‡æœ¬å†…å®¹è¿‡å°‘,è¯·æ£€æŸ¥æ–‡ä»¶è´¨é‡"
            }
        
        if not files:
            return {
                "is_valid": False,
                "error": "æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶"
            }
        
        return {"is_valid": True}
```

**Step 3: å®ç°GradingAgent**

```python
# backend/app/agents/grading_agent.py
import logging
from typing import Dict, List
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from app.agents.state import GradingState
from app.core.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class GradingAgent:
    """æ‰¹æ”¹Agent - è´Ÿè´£æ ¸å¿ƒæ‰¹æ”¹é€»è¾‘"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            api_key=settings.OPENAI_API_KEY,
            model="gpt-4-turbo-preview",
            temperature=0.3
        )
        self.name = "GradingAgent"
    
    async def process(self, state: GradingState) -> GradingState:
        """æ‰§è¡Œæ‰¹æ”¹æµç¨‹"""
        logger.info(f"[{self.name}] Starting grading for submission {state['submission_id']}")
        
        try:
            state["status"] = "grading"
            state["current_step"] = "grade"
            state["messages"].append({
                "agent": self.name,
                "action": "started",
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # 1. è·å–æ‰¹æ”¹æ ‡å‡†
            assignment = await self._get_assignment(state["assignment_id"])
            grading_standard = assignment["grading_standard"]
            
            # 2. æ„å»ºæ‰¹æ”¹æç¤ºè¯
            prompt = self._build_grading_prompt(
                student_answer=state["extracted_text"],
                grading_standard=grading_standard,
                config=state["config"]
            )
            
            # 3. è°ƒç”¨LLMè¿›è¡Œæ‰¹æ”¹
            logger.info(f"[{self.name}] Calling LLM for grading")
            response = await self.llm.ainvoke(prompt)
            
            # 4. è§£ææ‰¹æ”¹ç»“æœ
            grading_result = self._parse_grading_result(response.content)
            
            # 5. æ›´æ–°çŠ¶æ€
            state["score"] = grading_result["score"]
            state["max_score"] = assignment["max_score"]
            state["errors"] = grading_result["errors"]
            state["confidence"] = grading_result["confidence"]
            state["status"] = "graded"
            state["progress"] = 60.0
            
            state["messages"].append({
                "agent": self.name,
                "action": "completed",
                "timestamp": datetime.utcnow().isoformat(),
                "result": {
                    "score": grading_result["score"],
                    "error_count": len(grading_result["errors"]),
                    "confidence": grading_result["confidence"]
                }
            })
            
            logger.info(f"[{self.name}] Grading completed: score={grading_result['score']}")
            return state
            
        except Exception as e:
            logger.error(f"[{self.name}] Error during grading: {e}", exc_info=True)
            state["status"] = "failed"
            state["error_message"] = str(e)
            return state
    
    def _build_grading_prompt(
        self,
        student_answer: str,
        grading_standard: Dict,
        config: Dict
    ) -> str:
        """æ„å»ºæ‰¹æ”¹æç¤ºè¯"""
        strictness = config.get("strictness", "standard")
        
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™å¸ˆ,æ­£åœ¨æ‰¹æ”¹å­¦ç”Ÿä½œä¸šã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§è¯„åˆ†æ ‡å‡†è¿›è¡Œæ‰¹æ”¹,æ‰¾å‡ºå­¦ç”Ÿç­”æ¡ˆä¸­çš„é”™è¯¯,å¹¶ç»™å‡ºè¯¦ç»†çš„åé¦ˆã€‚

è¯„åˆ†ä¸¥æ ¼ç¨‹åº¦: {strictness}
- loose: å®½æ¾,æ³¨é‡é¼“åŠ±
- standard: æ ‡å‡†,å®¢è§‚å…¬æ­£
- strict: ä¸¥æ ¼,è¦æ±‚ç²¾ç¡®

è¯·ä»¥JSONæ ¼å¼è¾“å‡ºæ‰¹æ”¹ç»“æœã€‚"""),
            ("user", """
ã€æ‰¹æ”¹æ ‡å‡†ã€‘
{criteria}

ã€æ ‡å‡†ç­”æ¡ˆã€‘
{answer}

ã€å­¦ç”Ÿç­”æ¡ˆã€‘
{student_answer}

è¯·è¾“å‡ºJSONæ ¼å¼çš„æ‰¹æ”¹ç»“æœ,åŒ…å«ä»¥ä¸‹å­—æ®µ:
{{
    "score": åˆ†æ•°(0-100),
    "confidence": ç½®ä¿¡åº¦(0-1),
    "errors": [
        {{
            "type": "é”™è¯¯ç±»å‹",
            "location": "é”™è¯¯ä½ç½®",
            "description": "é”™è¯¯è¯´æ˜",
            "correct_answer": "æ­£ç¡®ç­”æ¡ˆ",
            "severity": "high|medium|low",
            "deduction": æ‰£åˆ†
        }}
    ],
    "overall_comment": "æ€»ä½“è¯„ä»·",
    "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2"],
    "weaknesses": ["ä¸è¶³1", "ä¸è¶³2"]
}}
""")
        ])
        
        return prompt_template.format_messages(
            strictness=strictness,
            criteria=grading_standard["criteria"],
            answer=grading_standard["answer"],
            student_answer=student_answer
        )
    
    def _parse_grading_result(self, response: str) -> Dict:
        """è§£ææ‰¹æ”¹ç»“æœ"""
        import json
        
        try:
            # æå–JSONéƒ¨åˆ†
            start_idx = response.find("{")
            end_idx = response.rfind("}") + 1
            json_str = response[start_idx:end_idx]
            
            result = json.loads(json_str)
            return result
            
        except Exception as e:
            logger.error(f"Failed to parse grading result: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return {
                "score": 0,
                "confidence": 0.5,
                "errors": [],
                "overall_comment": "æ‰¹æ”¹ç»“æœè§£æå¤±è´¥",
                "strengths": [],
                "weaknesses": []
            }
    
    async def _get_assignment(self, assignment_id: UUID) -> Dict:
        """è·å–ä½œä¸šä¿¡æ¯"""
        from app.services.assignment_service import AssignmentService
        from app.core.database import get_db_session
        
        async with get_db_session() as db:
            service = AssignmentService(db)
            assignment = await service.get_assignment(assignment_id)
            return assignment
```

**Step 4: å®ç°Orchestrator**

```python
# backend/app/agents/orchestrator.py
import logging
from typing import Dict
from langgraph.graph import StateGraph, END

from app.agents.state import GradingState
from app.agents.preprocess_agent import PreprocessAgent
from app.agents.grading_agent import GradingAgent
from app.agents.review_agent import ReviewAgent
from app.agents.feedback_agent import FeedbackAgent

logger = logging.getLogger(__name__)


class OrchestratorAgent:
    """ç¼–æ’å™¨Agent - è´Ÿè´£æ•´ä½“æµç¨‹æ§åˆ¶"""
    
    def __init__(self):
        self.preprocess_agent = PreprocessAgent()
        self.grading_agent = GradingAgent()
        self.review_agent = ReviewAgent()
        self.feedback_agent = FeedbackAgent()
        
        self.workflow = self._build_workflow()
    
    def _build_workflow(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        workflow = StateGraph(GradingState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("preprocess", self.preprocess_agent.process)
        workflow.add_node("grade", self.grading_agent.process)
        workflow.add_node("review", self.review_agent.process)
        workflow.add_node("feedback", self.feedback_agent.process)
        
        # å®šä¹‰æµç¨‹
        workflow.set_entry_point("preprocess")
        
        # é¢„å¤„ç† -> æ‰¹æ”¹
        workflow.add_conditional_edges(
            "preprocess",
            self._check_preprocess_status,
            {
                "continue": "grade",
                "failed": END
            }
        )
        
        # æ‰¹æ”¹ -> å®¡æ ¸æˆ–åé¦ˆ
        workflow.add_conditional_edges(
            "grade",
            self._should_review,
            {
                "review": "review",
                "feedback": "feedback",
                "failed": END
            }
        )
        
        # å®¡æ ¸ -> åé¦ˆ
        workflow.add_conditional_edges(
            "review",
            self._check_review_status,
            {
                "continue": "feedback",
                "failed": END
            }
        )
        
        # åé¦ˆ -> ç»“æŸ
        workflow.add_edge("feedback", END)
        
        return workflow.compile()
    
    def _check_preprocess_status(self, state: GradingState) -> str:
        """æ£€æŸ¥é¢„å¤„ç†çŠ¶æ€"""
        if state["status"] == "failed":
            return "failed"
        return "continue"
    
    def _should_review(self, state: GradingState) -> str:
        """å†³å®šæ˜¯å¦éœ€è¦å®¡æ ¸"""
        if state["status"] == "failed":
            return "failed"
        
        # å¦‚æœé…ç½®å¯ç”¨å®¡æ ¸ä¸”ç½®ä¿¡åº¦è¾ƒä½,åˆ™è¿›è¡Œå®¡æ ¸
        if state["config"].get("enable_review") and state.get("confidence", 1.0) < 0.8:
            return "review"
        
        return "feedback"
    
    def _check_review_status(self, state: GradingState) -> str:
        """æ£€æŸ¥å®¡æ ¸çŠ¶æ€"""
        if state["status"] == "failed":
            return "failed"
        return "continue"
    
    async def execute(self, input_data: Dict) -> Dict:
        """æ‰§è¡Œæ‰¹æ”¹æµç¨‹"""
        logger.info(f"[Orchestrator] Starting grading workflow for submission {input_data['submission_id']}")
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = GradingState(
            submission_id=input_data["submission_id"],
            assignment_id=input_data["assignment_id"],
            student_id=input_data["student_id"],
            status="pending",
            current_step="",
            progress=0.0,
            messages=[],
            preprocessed_files=None,
            extracted_text=None,
            file_metadata=None,
            score=None,
            max_score=100,
            errors=None,
            annotations=None,
            confidence=None,
            review_passed=None,
            review_comments=None,
            adjusted_score=None,
            feedback_text=None,
            suggestions=None,
            knowledge_points=None,
            config=input_data.get("config", {}),
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow(),
            error_message=None
        )
        
        # æ‰§è¡Œå·¥ä½œæµ
        result = await self.workflow.ainvoke(initial_state)
        
        logger.info(f"[Orchestrator] Workflow completed with status: {result['status']}")
        return result
```

---

## 2. APIé›†æˆ

```python
# backend/app/api/grading.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession

from app.core.database import get_db
from app.agents.orchestrator import OrchestratorAgent
from app.schemas.grading import GradingRequest, GradingResponse

router = APIRouter(prefix="/api/v1/grading", tags=["grading"])


@router.post("/submit", response_model=GradingResponse)
async def submit_grading(
    request: GradingRequest,
    db: AsyncSession = Depends(get_db)
):
    """æäº¤æ‰¹æ”¹è¯·æ±‚"""
    try:
        # åˆ›å»ºOrchestrator
        orchestrator = OrchestratorAgent()
        
        # æ‰§è¡Œæ‰¹æ”¹
        result = await orchestrator.execute({
            "submission_id": request.submission_id,
            "assignment_id": request.assignment_id,
            "student_id": request.student_id,
            "config": {
                "strictness": request.strictness,
                "enable_review": request.enable_review,
                "enable_analytics": request.enable_analytics
            }
        })
        
        return GradingResponse(
            success=result["status"] == "completed",
            submission_id=result["submission_id"],
            score=result.get("score"),
            feedback=result.get("feedback_text"),
            errors=result.get("errors", []),
            processing_time=result.get("processing_time")
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

**ç»§ç»­é˜…è¯»**: æŸ¥çœ‹å…¶ä»–è®¾è®¡æ–‡æ¡£äº†è§£æ›´å¤šå®ç°ç»†èŠ‚

